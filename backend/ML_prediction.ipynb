{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegressionCV was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "from typing import Tuple, List, Union\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MLModel:\n",
    "    def __init__(self, model_path: str = 'ml_model.pkl'):\n",
    "        \"\"\"Initialize the ML model with configuration.\"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        self.numeric_features = None\n",
    "        self.categorical_features = None\n",
    "\n",
    "    def identify_feature_types(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Identify numeric and categorical columns.\"\"\"\n",
    "        numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Remove target variable if present\n",
    "        if 'Growth_Milestone' in numeric_features:\n",
    "            numeric_features.remove('Growth_Milestone')\n",
    "        if 'Growth_Milestone' in categorical_features:\n",
    "            categorical_features.remove('Growth_Milestone')\n",
    "            \n",
    "        return numeric_features, categorical_features\n",
    "\n",
    "    def create_preprocessor(self) -> ColumnTransformer:\n",
    "        \"\"\"Create a preprocessing pipeline for both numeric and categorical features.\"\"\"\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        return ColumnTransformer(transformers=[\n",
    "            ('num', numeric_transformer, self.numeric_features),\n",
    "            ('cat', categorical_transformer, self.categorical_features)\n",
    "        ])\n",
    "\n",
    "    def load_and_preprocess_data(self, file_path: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Load and preprocess the data.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "            logger.info(f\"Data loaded successfully from {file_path}\")\n",
    "            \n",
    "            # Identify features\n",
    "            self.numeric_features, self.categorical_features = self.identify_feature_types(df)\n",
    "            \n",
    "            # Split features and target\n",
    "            X = df.drop('Growth_Milestone', axis=1)\n",
    "            y = df['Growth_Milestone']\n",
    "            \n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, file_path: str) -> None:\n",
    "        \"\"\"Train the model and save it.\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess data\n",
    "            X, y = self.load_and_preprocess_data(file_path)\n",
    "            \n",
    "            # Create train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Create preprocessing pipeline\n",
    "            self.preprocessor = self.create_preprocessor()\n",
    "            \n",
    "            # Create and train pipeline\n",
    "            self.model = Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', LogisticRegression(\n",
    "                    max_iter=1000,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42\n",
    "                ))\n",
    "            ])\n",
    "            \n",
    "            # Fit the model\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate model\n",
    "            train_score = self.model.score(X_train, y_train)\n",
    "            test_score = self.model.score(X_test, y_test)\n",
    "            cv_scores = cross_val_score(self.model, X, y, cv=5)\n",
    "            \n",
    "            logger.info(f\"Train accuracy: {train_score:.4f}\")\n",
    "            logger.info(f\"Test accuracy: {test_score:.4f}\")\n",
    "            logger.info(f\"Cross-validation scores: mean={cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "            \n",
    "            # Save the model\n",
    "            joblib.dump(self.model, self.model_path)\n",
    "            logger.info(f\"Model saved to {self.model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, data_array: Union[List, np.ndarray]) -> int:\n",
    "        \"\"\"Make predictions using the trained model.\"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                self.model = joblib.load(self.model_path)\n",
    "                logger.info(\"Model loaded successfully\")\n",
    "            \n",
    "            # Convert input to DataFrame with feature names\n",
    "            data_df = pd.DataFrame([data_array], columns=self.numeric_features + self.categorical_features)\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = self.model.predict(data_df)[0]\n",
    "            logger.info(f\"Prediction made successfully: {prediction}\")\n",
    "            \n",
    "            return prediction\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during prediction: {str(e)}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
